{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7a1b31-10de-4d73-9b81-2d994381480f",
   "metadata": {},
   "source": [
    "#### [ML] ViT(20.10); Vision Transformer 코드 구현 및 설명 with pytorch\n",
    "https://kimbg.tistory.com/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c2fe5c-528f-4d60-bd45-752c616314aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f6b3cd6-a71a-4594-b801-833e9f24a042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
      "               RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "img_size = 224\n",
    "\n",
    "# Define image size\n",
    "image_size = (img_size, img_size)  # Replace with your desired image dimensions\n",
    "\n",
    "# Create data augmentation transforms\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor (CHW format)\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize pixel values (common practice)\n",
    "    transforms.Resize(image_size),  # Resize image to specified dimensions\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n",
    "    transforms.RandomRotation(degrees=(-15, 15)),  # Random rotation with range -15 to 15 degrees\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0), ratio=(0.75, 1.3333))  # Random resized crop\n",
    "])\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=data_augmentation)\n",
    "\n",
    "test_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=data_augmentation)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False)\n",
    "\n",
    "print(train_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcfa7ff9-ea41-4385-bdb6-5f84dcbf619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 3, 224, 224]) type: torch.FloatTensor\n",
      "Y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (X_train, Y_train) in train_loader:\n",
    "    print(f\"X_train: {X_train.size()} type: {X_train.type()}\")\n",
    "    print(f\"Y_train: {Y_train.size()} type: {Y_train.type()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e69e8322-99f9-415b-85fd-6a9d95e297dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 3, 224, 224]) type: torch.FloatTensor\n",
      "Y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (x, Y_train) in train_loader:\n",
    "    print(f\"X_train: {x.size()} type: {x.type()}\")\n",
    "    print(f\"Y_train: {Y_train.size()} type: {Y_train.type()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e0f5739-41b7-4fe8-aba8-9244a4ea4e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train : torch.Size([32, 3, 224, 224])\n",
      "patches : torch.Size([32, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "## input ##\n",
    "print('X_train :', X_train.shape)\n",
    "\n",
    "patch_size = 16 # 16x16 pixel patch\n",
    "patches = rearrange(X_train, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', \n",
    "                    s1=patch_size, s2=patch_size)\n",
    "print('patches :', patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc7bf73-1763-4f05-84eb-cee9a64da951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : torch.Size([32, 3, 224, 224])\n",
      "patches : torch.Size([32, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "## input ##\n",
    "x = torch.randn(32, 3, 224, 224)\n",
    "print('x :', x.shape)\n",
    "\n",
    "patch_size = 16 # 16x16 pixel patch\n",
    "patches = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', \n",
    "                    s1=patch_size, s2=patch_size)\n",
    "print('patches :', patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc57bc2-04dc-4743-80aa-b55e41cb897c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768, 14, 14])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(32, 3, 224, 224)\n",
    "\n",
    "patch_size = 16\n",
    "in_channels = 3\n",
    "emb_size = 768 # channel * patch_size * patch_size\n",
    "\n",
    "conv_t = nn.Conv2d(in_channels, emb_size, \n",
    "              kernel_size=patch_size, stride=patch_size)(t)\n",
    "conv_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f83704f4-242a-4e0d-9d1c-0255b21b0176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 196, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = rearrange(conv_t, 'b e (h) (w) -> b (h w) e')\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a7fa9f7-e80a-4626-ba3b-f8d5ec6d1a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 3, 224, 224]) type: torch.FloatTensor\n",
      "Y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (x, Y_train) in train_loader:\n",
    "    print(f\"X_train: {x.size()} type: {x.type()}\")\n",
    "    print(f\"Y_train: {Y_train.size()} type: {Y_train.type()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f866bde-a9e3-45e3-8331-5fb0ae8fbaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d2c445-6122-41f3-825c-fefa102eef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "patch_size = 16\n",
    "in_channels = 3\n",
    "emb_size = 768 # channel * patch_size * patch_size\n",
    "\n",
    "# using a conv layer instead of a linear one -> performance gains\n",
    "projection = nn.Sequential(\n",
    "    nn.Conv2d(in_channels, emb_size, \n",
    "              kernel_size=patch_size, stride=patch_size), # torch.Size([8, 768, 14, 14])\n",
    "    Rearrange('b e (h) (w) -> b (h w) e'))\n",
    "\n",
    "summary(projection, x.shape[1:], device='cpu')\n",
    "# Conv2d parameter size:\n",
    "# 590592 = 16 * 16 (patch_size) * 768 (out_channels) * 3 (in_channels) + 768 (bais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78821eb1-9f1b-40a7-80c0-676b3c6d65ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected X shape : torch.Size([32, 196, 768])\n",
      "Cls Shape : torch.Size([1, 1, 768]) , Pos Shape : torch.Size([197, 768])\n"
     ]
    }
   ],
   "source": [
    "emb_size = 768\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "\n",
    "# 이미지를 패치사이즈로 나누고 flatten\n",
    "projected_x = projection(x)\n",
    "print('Projected X shape :', projected_x.shape)\n",
    "\n",
    "# cls_token과 pos encoding Parameter 정의\n",
    "cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "print('Cls Shape :', cls_token.shape, ', Pos Shape :', positions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b08f5244-82b1-4ff5-ae3a-ed1a0289238a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated Cls shape : torch.Size([32, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "# cls_token을 반복하여 배치사이즈의 크기와 맞춰줌\n",
    "batch_size = 32\n",
    "cls_tokens = repeat(cls_token, '() n e -> b n e', b=batch_size)\n",
    "print('Repeated Cls shape :', cls_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a87eeae-d79b-48f8-9d1f-f4b5bb1c9888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 197, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cls_token과 projected_x를 concatenate\n",
    "# cls_token: [8, 1, 768], projected_x : [8, 196, 768] \n",
    "cat_x = torch.cat([cls_tokens, projected_x], dim=1)\n",
    "cat_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89d10202-65d5-4c29-afe6-19affc719ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([197, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fabea761-d0c5-4b80-adbf-24b8ff4a3324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output :  torch.Size([32, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# position encoding을 더해줌\n",
    "cat_x += positions\n",
    "print('output : ', cat_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "945da7a2-419d-43d1-b86f-0142b4a6fea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,2) + torch.ones(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac1b9bb5-bbe5-4799-b1d0-e924add20b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, \n",
    "                 emb_size: int = 768, img_size: int = 224):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Sequential(\n",
    "            # using a conv layer instead of a linear one -> performance gains\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        # prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        # add position embedding\n",
    "        x += self.positions\n",
    "\n",
    "        return x\n",
    "\n",
    "PE = PatchEmbedding()\n",
    "summary(PE, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a59964a3-87e2-4d8e-a66d-feb9d2fad69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True) Linear(in_features=768, out_features=768, bias=True) Linear(in_features=768, out_features=768, bias=True)\n",
      "torch.Size([32, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "#x = torch.randn(8, 3, 224, 224)\n",
    "emb_size = 768\n",
    "num_heads = 8\n",
    "\n",
    "keys = nn.Linear(emb_size, emb_size)\n",
    "queries = nn.Linear(emb_size, emb_size)\n",
    "values = nn.Linear(emb_size, emb_size)\n",
    "print(keys, queries, values)\n",
    "\n",
    "#print(x.shape)\n",
    "x = PE(x)\n",
    "print(queries(x).shape) # batch, n, emb_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d42dc722-53b2-470c-b303-5e5d424b3d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 197, 96])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = rearrange(queries(x), \"b n (h d) -> b h n d\", h=num_heads) # -> batch, head, n, emb_size/head\n",
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f5b35a6-0900-4683-a7fd-abff959ad227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape : torch.Size([32, 8, 197, 96]) torch.Size([32, 8, 197, 96]) torch.Size([32, 8, 197, 96])\n"
     ]
    }
   ],
   "source": [
    "keys = rearrange(keys(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "values  = rearrange(values(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "\n",
    "print('shape :', queries.shape, keys.shape, values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80a143ee-6282-419a-86ca-c6d160bcc150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy : torch.Size([32, 8, 197, 197])\n"
     ]
    }
   ],
   "source": [
    "# Queries * Keys\n",
    "energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "print('energy :', energy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98ba25e1-1d04-4fdf-a1b3-ae8f25923714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att : torch.Size([32, 8, 197, 197])\n"
     ]
    }
   ],
   "source": [
    "# Get Attention Score\n",
    "scaling = emb_size ** (1/2)\n",
    "att = F.softmax(energy/scaling, dim=-1) \n",
    "print('att :', att.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b549189-032f-4c37-853e-f7d16165b2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out : torch.Size([32, 8, 197, 96])\n"
     ]
    }
   ],
   "source": [
    "# Attention Score * values\n",
    "out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "print('out :', out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02c9a57f-3136-4e94-8be2-ed321567b181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out2 :  torch.Size([32, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# Rearrage to emb_size\n",
    "out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "print('out2 : ', out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "119f5071-d8ce-4ff2-ae73-d60e9cc37bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "X_train: torch.Size([32, 3, 224, 224]) type: torch.FloatTensor\n",
      "Y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "img_size = 224\n",
    "\n",
    "# Define image size\n",
    "image_size = (img_size, img_size)  # Replace with your desired image dimensions\n",
    "\n",
    "# Create data augmentation transforms\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor (CHW format)\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize pixel values (common practice)\n",
    "    transforms.Resize(image_size),  # Resize image to specified dimensions\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n",
    "    transforms.RandomRotation(degrees=(-15, 15)),  # Random rotation with range -15 to 15 degrees\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0), ratio=(0.75, 1.3333))  # Random resized crop\n",
    "])\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=data_augmentation)\n",
    "\n",
    "test_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=data_augmentation)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False)\n",
    "\n",
    "for (x, Y_train) in train_loader:\n",
    "    print(f\"X_train: {x.size()} type: {x.type()}\")\n",
    "    print(f\"Y_train: {Y_train.size()} type: {Y_train.type()}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cd9dc00-6de8-47b1-8da0-349c21bf9cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 197, 768])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-2          [-1, 8, 197, 197]               0\n",
      "            Linear-3             [-1, 197, 768]         590,592\n",
      "================================================================\n",
      "Total params: 2,362,368\n",
      "Trainable params: 2,362,368\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.58\n",
      "Forward/backward pass size (MB): 6.99\n",
      "Params size (MB): 9.01\n",
      "Estimated Total Size (MB): 16.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "            \n",
    "        scaling = self.emb_size ** (1/2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "#x = torch.randn(8, 3, 224, 224)\n",
    "PE = PatchEmbedding()\n",
    "print(x.shape)\n",
    "x = PE(x)\n",
    "print(x.shape)\n",
    "MHA = MultiHeadAttention()\n",
    "summary(MHA, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf2405ad-31e8-4a4f-9fd2-508708d273ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 197, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a23df751-961b-4633-b415-3a3a2eb7975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 197, 2304])\n",
      "torch.Size([3, 32, 8, 197, 96])\n"
     ]
    }
   ],
   "source": [
    "qkv = MHA.qkv(x)\n",
    "print(qkv.shape)\n",
    "qkv = rearrange(qkv, \"b n (h d qkv) -> (qkv) b h n d\", h=8, qkv=3)\n",
    "print(qkv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75a107f7-a4a7-495f-9830-12f131d4d1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "X_train: torch.Size([32, 3, 224, 224]) type: torch.FloatTensor\n",
      "Y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "img_size = 224\n",
    "\n",
    "# Define image size\n",
    "image_size = (img_size, img_size)  # Replace with your desired image dimensions\n",
    "\n",
    "# Create data augmentation transforms\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor (CHW format)\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize pixel values (common practice)\n",
    "    transforms.Resize(image_size),  # Resize image to specified dimensions\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n",
    "    transforms.RandomRotation(degrees=(-15, 15)),  # Random rotation with range -15 to 15 degrees\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0), ratio=(0.75, 1.3333))  # Random resized crop\n",
    "])\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=data_augmentation)\n",
    "\n",
    "test_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=data_augmentation)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False)\n",
    "\n",
    "for (x, Y_train) in train_loader:\n",
    "    print(f\"X_train: {x.size()} type: {x.type()}\")\n",
    "    print(f\"Y_train: {Y_train.size()} type: {Y_train.type()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "394690e2-067f-4f7c-98ed-6b21e85e0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1             [-1, 197, 768]           1,536\n",
      "            Linear-2            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-3          [-1, 8, 197, 197]               0\n",
      "            Linear-4             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-5             [-1, 197, 768]               0\n",
      "           Dropout-6             [-1, 197, 768]               0\n",
      "       ResidualAdd-7             [-1, 197, 768]               0\n",
      "         LayerNorm-8             [-1, 197, 768]           1,536\n",
      "            Linear-9            [-1, 197, 3072]       2,362,368\n",
      "             GELU-10            [-1, 197, 3072]               0\n",
      "          Dropout-11            [-1, 197, 3072]               0\n",
      "           Linear-12             [-1, 197, 768]       2,360,064\n",
      "          Dropout-13             [-1, 197, 768]               0\n",
      "      ResidualAdd-14             [-1, 197, 768]               0\n",
      "================================================================\n",
      "Total params: 7,087,872\n",
      "Trainable params: 7,087,872\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.58\n",
      "Forward/backward pass size (MB): 30.07\n",
      "Params size (MB): 27.04\n",
      "Estimated Total Size (MB): 57.69\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 768,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "#x = torch.randn(8, 3, 224, 224)\n",
    "x = PE(x)\n",
    "x = MHA(x)\n",
    "TE = TransformerEncoderBlock()\n",
    "summary(TE, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9f69a79-0577-49d3-b78d-3e75360cff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(representation.shape) # torch.Size([8, 197, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61601513-f230-4e8c-95a9-9846e72e740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cls_head = reduce(representation, 'b n e -> b e', reduction='mean')\n",
    "#print(cls_head.shape)\n",
    "#cls_head = nn.Linear(768, 1000)(cls_head)\n",
    "#print(cls_head.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b6ecf9e-23ea-4ff3-93b4-92c5e30778cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "tr_encoder = TransformerEncoder()\n",
    "#print(tr_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ebe9f13-c96c-479e-b79d-2bfba771b4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "    PatchEmbedding-3             [-1, 197, 768]               0\n",
      "         LayerNorm-4             [-1, 197, 768]           1,536\n",
      "            Linear-5            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-6          [-1, 8, 197, 197]               0\n",
      "            Linear-7             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-8             [-1, 197, 768]               0\n",
      "           Dropout-9             [-1, 197, 768]               0\n",
      "      ResidualAdd-10             [-1, 197, 768]               0\n",
      "        LayerNorm-11             [-1, 197, 768]           1,536\n",
      "           Linear-12            [-1, 197, 3072]       2,362,368\n",
      "             GELU-13            [-1, 197, 3072]               0\n",
      "          Dropout-14            [-1, 197, 3072]               0\n",
      "           Linear-15             [-1, 197, 768]       2,360,064\n",
      "          Dropout-16             [-1, 197, 768]               0\n",
      "      ResidualAdd-17             [-1, 197, 768]               0\n",
      "        LayerNorm-18             [-1, 197, 768]           1,536\n",
      "           Linear-19            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-20          [-1, 8, 197, 197]               0\n",
      "           Linear-21             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-22             [-1, 197, 768]               0\n",
      "          Dropout-23             [-1, 197, 768]               0\n",
      "      ResidualAdd-24             [-1, 197, 768]               0\n",
      "        LayerNorm-25             [-1, 197, 768]           1,536\n",
      "           Linear-26            [-1, 197, 3072]       2,362,368\n",
      "             GELU-27            [-1, 197, 3072]               0\n",
      "          Dropout-28            [-1, 197, 3072]               0\n",
      "           Linear-29             [-1, 197, 768]       2,360,064\n",
      "          Dropout-30             [-1, 197, 768]               0\n",
      "      ResidualAdd-31             [-1, 197, 768]               0\n",
      "        LayerNorm-32             [-1, 197, 768]           1,536\n",
      "           Linear-33            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-34          [-1, 8, 197, 197]               0\n",
      "           Linear-35             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-36             [-1, 197, 768]               0\n",
      "          Dropout-37             [-1, 197, 768]               0\n",
      "      ResidualAdd-38             [-1, 197, 768]               0\n",
      "        LayerNorm-39             [-1, 197, 768]           1,536\n",
      "           Linear-40            [-1, 197, 3072]       2,362,368\n",
      "             GELU-41            [-1, 197, 3072]               0\n",
      "          Dropout-42            [-1, 197, 3072]               0\n",
      "           Linear-43             [-1, 197, 768]       2,360,064\n",
      "          Dropout-44             [-1, 197, 768]               0\n",
      "      ResidualAdd-45             [-1, 197, 768]               0\n",
      "        LayerNorm-46             [-1, 197, 768]           1,536\n",
      "           Linear-47            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-48          [-1, 8, 197, 197]               0\n",
      "           Linear-49             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-50             [-1, 197, 768]               0\n",
      "          Dropout-51             [-1, 197, 768]               0\n",
      "      ResidualAdd-52             [-1, 197, 768]               0\n",
      "        LayerNorm-53             [-1, 197, 768]           1,536\n",
      "           Linear-54            [-1, 197, 3072]       2,362,368\n",
      "             GELU-55            [-1, 197, 3072]               0\n",
      "          Dropout-56            [-1, 197, 3072]               0\n",
      "           Linear-57             [-1, 197, 768]       2,360,064\n",
      "          Dropout-58             [-1, 197, 768]               0\n",
      "      ResidualAdd-59             [-1, 197, 768]               0\n",
      "        LayerNorm-60             [-1, 197, 768]           1,536\n",
      "           Linear-61            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-62          [-1, 8, 197, 197]               0\n",
      "           Linear-63             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-64             [-1, 197, 768]               0\n",
      "          Dropout-65             [-1, 197, 768]               0\n",
      "      ResidualAdd-66             [-1, 197, 768]               0\n",
      "        LayerNorm-67             [-1, 197, 768]           1,536\n",
      "           Linear-68            [-1, 197, 3072]       2,362,368\n",
      "             GELU-69            [-1, 197, 3072]               0\n",
      "          Dropout-70            [-1, 197, 3072]               0\n",
      "           Linear-71             [-1, 197, 768]       2,360,064\n",
      "          Dropout-72             [-1, 197, 768]               0\n",
      "      ResidualAdd-73             [-1, 197, 768]               0\n",
      "        LayerNorm-74             [-1, 197, 768]           1,536\n",
      "           Linear-75            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-76          [-1, 8, 197, 197]               0\n",
      "           Linear-77             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-78             [-1, 197, 768]               0\n",
      "          Dropout-79             [-1, 197, 768]               0\n",
      "      ResidualAdd-80             [-1, 197, 768]               0\n",
      "        LayerNorm-81             [-1, 197, 768]           1,536\n",
      "           Linear-82            [-1, 197, 3072]       2,362,368\n",
      "             GELU-83            [-1, 197, 3072]               0\n",
      "          Dropout-84            [-1, 197, 3072]               0\n",
      "           Linear-85             [-1, 197, 768]       2,360,064\n",
      "          Dropout-86             [-1, 197, 768]               0\n",
      "      ResidualAdd-87             [-1, 197, 768]               0\n",
      "        LayerNorm-88             [-1, 197, 768]           1,536\n",
      "           Linear-89            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-90          [-1, 8, 197, 197]               0\n",
      "           Linear-91             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-92             [-1, 197, 768]               0\n",
      "          Dropout-93             [-1, 197, 768]               0\n",
      "      ResidualAdd-94             [-1, 197, 768]               0\n",
      "        LayerNorm-95             [-1, 197, 768]           1,536\n",
      "           Linear-96            [-1, 197, 3072]       2,362,368\n",
      "             GELU-97            [-1, 197, 3072]               0\n",
      "          Dropout-98            [-1, 197, 3072]               0\n",
      "           Linear-99             [-1, 197, 768]       2,360,064\n",
      "         Dropout-100             [-1, 197, 768]               0\n",
      "     ResidualAdd-101             [-1, 197, 768]               0\n",
      "       LayerNorm-102             [-1, 197, 768]           1,536\n",
      "          Linear-103            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-104          [-1, 8, 197, 197]               0\n",
      "          Linear-105             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-106             [-1, 197, 768]               0\n",
      "         Dropout-107             [-1, 197, 768]               0\n",
      "     ResidualAdd-108             [-1, 197, 768]               0\n",
      "       LayerNorm-109             [-1, 197, 768]           1,536\n",
      "          Linear-110            [-1, 197, 3072]       2,362,368\n",
      "            GELU-111            [-1, 197, 3072]               0\n",
      "         Dropout-112            [-1, 197, 3072]               0\n",
      "          Linear-113             [-1, 197, 768]       2,360,064\n",
      "         Dropout-114             [-1, 197, 768]               0\n",
      "     ResidualAdd-115             [-1, 197, 768]               0\n",
      "       LayerNorm-116             [-1, 197, 768]           1,536\n",
      "          Linear-117            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-118          [-1, 8, 197, 197]               0\n",
      "          Linear-119             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-120             [-1, 197, 768]               0\n",
      "         Dropout-121             [-1, 197, 768]               0\n",
      "     ResidualAdd-122             [-1, 197, 768]               0\n",
      "       LayerNorm-123             [-1, 197, 768]           1,536\n",
      "          Linear-124            [-1, 197, 3072]       2,362,368\n",
      "            GELU-125            [-1, 197, 3072]               0\n",
      "         Dropout-126            [-1, 197, 3072]               0\n",
      "          Linear-127             [-1, 197, 768]       2,360,064\n",
      "         Dropout-128             [-1, 197, 768]               0\n",
      "     ResidualAdd-129             [-1, 197, 768]               0\n",
      "       LayerNorm-130             [-1, 197, 768]           1,536\n",
      "          Linear-131            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-132          [-1, 8, 197, 197]               0\n",
      "          Linear-133             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-134             [-1, 197, 768]               0\n",
      "         Dropout-135             [-1, 197, 768]               0\n",
      "     ResidualAdd-136             [-1, 197, 768]               0\n",
      "       LayerNorm-137             [-1, 197, 768]           1,536\n",
      "          Linear-138            [-1, 197, 3072]       2,362,368\n",
      "            GELU-139            [-1, 197, 3072]               0\n",
      "         Dropout-140            [-1, 197, 3072]               0\n",
      "          Linear-141             [-1, 197, 768]       2,360,064\n",
      "         Dropout-142             [-1, 197, 768]               0\n",
      "     ResidualAdd-143             [-1, 197, 768]               0\n",
      "       LayerNorm-144             [-1, 197, 768]           1,536\n",
      "          Linear-145            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-146          [-1, 8, 197, 197]               0\n",
      "          Linear-147             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-148             [-1, 197, 768]               0\n",
      "         Dropout-149             [-1, 197, 768]               0\n",
      "     ResidualAdd-150             [-1, 197, 768]               0\n",
      "       LayerNorm-151             [-1, 197, 768]           1,536\n",
      "          Linear-152            [-1, 197, 3072]       2,362,368\n",
      "            GELU-153            [-1, 197, 3072]               0\n",
      "         Dropout-154            [-1, 197, 3072]               0\n",
      "          Linear-155             [-1, 197, 768]       2,360,064\n",
      "         Dropout-156             [-1, 197, 768]               0\n",
      "     ResidualAdd-157             [-1, 197, 768]               0\n",
      "       LayerNorm-158             [-1, 197, 768]           1,536\n",
      "          Linear-159            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-160          [-1, 8, 197, 197]               0\n",
      "          Linear-161             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-162             [-1, 197, 768]               0\n",
      "         Dropout-163             [-1, 197, 768]               0\n",
      "     ResidualAdd-164             [-1, 197, 768]               0\n",
      "       LayerNorm-165             [-1, 197, 768]           1,536\n",
      "          Linear-166            [-1, 197, 3072]       2,362,368\n",
      "            GELU-167            [-1, 197, 3072]               0\n",
      "         Dropout-168            [-1, 197, 3072]               0\n",
      "          Linear-169             [-1, 197, 768]       2,360,064\n",
      "         Dropout-170             [-1, 197, 768]               0\n",
      "     ResidualAdd-171             [-1, 197, 768]               0\n",
      "          Reduce-172                  [-1, 768]               0\n",
      "       LayerNorm-173                  [-1, 768]           1,536\n",
      "          Linear-174                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 86,415,592\n",
      "Trainable params: 86,415,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 364.33\n",
      "Params size (MB): 329.65\n",
      "Estimated Total Size (MB): 694.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))\n",
    "\n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 768,\n",
    "                img_size: int = 224,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = 1000,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n",
    "        \n",
    "summary(ViT(), (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02d2d108-4acd-41a6-a27f-81f6c4e200ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "X_train: torch.Size([32, 3, 224, 224]) type: torch.FloatTensor\n",
      "Y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "img_size = 224\n",
    "\n",
    "# Define image size\n",
    "image_size = (img_size, img_size)  # Replace with your desired image dimensions\n",
    "\n",
    "# Create data augmentation transforms\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor (CHW format)\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize pixel values (common practice)\n",
    "    transforms.Resize(image_size),  # Resize image to specified dimensions\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n",
    "    transforms.RandomRotation(degrees=(-15, 15)),  # Random rotation with range -15 to 15 degrees\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0), ratio=(0.75, 1.3333))  # Random resized crop\n",
    "])\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=data_augmentation)\n",
    "\n",
    "test_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=data_augmentation)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False)\n",
    "\n",
    "for (x, Y_train) in train_loader:\n",
    "    print(f\"X_train: {x.size()} type: {x.type()}\")\n",
    "    print(f\"Y_train: {Y_train.size()} type: {Y_train.type()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6021b-0388-401c-984a-6b1290f0c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT()\n",
    "out = vit(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68325939-d50e-4e84-b1f4-7113a67350e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### cifar100_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc3788-696b-4838-8267-bec87749ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = datasets.CIFAR10(root=\"./data/\",\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=\"./data/\",\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False)\n",
    "\n",
    "print(train_loader.dataset)\n",
    "\n",
    "for (X_train, Y_train) in train_loader:\n",
    "    print(f\"X_train: {X_train.size()} type: {X_train.type()}\")\n",
    "    print(f\"Y_train: {Y_train.size()} type: {Y_train.type()}\")\n",
    "    break\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=8,\n",
    "            kernel_size=3,\n",
    "            padding=1)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            padding=1)\n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "        self.fc1 = nn.Linear(8 * 8 * 16, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 8 * 8 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using PyTorch version: {torch.__version__}, Device: {DEVICE}\")\n",
    "\n",
    "model = CNN().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                f\"train Epoch: {Epoch} [{batch_idx * len(image)}/{len(train_loader.dataset)}({100. * batch_idx / len(train_loader):.0f}%)]\\tTrain Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim=True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "EPOCHS = 10\n",
    "for Epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval=200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(f\"\\n[EPOCH: {Epoch}]\\tTest Loss: {test_loss:.4f}\\tTest Accuracy: {test_accuracy} % \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1d125c4-1081-4432-9bc7-aaec39d460aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "y_true = torch.tensor([0, 1, 2, 2]).cuda().cpu()\n",
    "y_score = torch.tensor([[0.5, 0.2, 0.2],  # 0 is in top 2\n",
    "                    [0.3, 0.4, 0.2],  # 1 is in top 2\n",
    "                    [0.2, 0.4, 0.3],  # 2 is in top 2\n",
    "                    [0.7, 0.2, 0.1]]).cuda().cpu() # 2 isn't in top 2\n",
    "print(top_k_accuracy_score(y_true, y_score, k=1))\n",
    "# Not normalizing gives the number of \"correctly\" classified samples\n",
    "print(top_k_accuracy_score(y_true, y_score, k=1, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e3739fc-fe8b-4197-a45e-14103d6e8ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 accuracy: 1.0\n",
      "Top-3 accuracy: 4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "y_true = torch.tensor([0, 1, 2, 2]).cuda()\n",
    "y_pred = torch.tensor([[0.5, 0.2, 0.2],  # 0 is in top 2\n",
    "                    [0.3, 0.4, 0.2],  # 1 is in top 2\n",
    "                    [0.2, 0.4, 0.3],  # 2 is in top 2\n",
    "                    [0.7, 0.2, 0.1]]).cuda() # 2 isn't in top 2\n",
    "\n",
    "k = 3\n",
    "\n",
    "def top_k_accuracy(y_true, y_pred, k):\n",
    "  \"\"\"\n",
    "  Calculates top-k accuracy.\n",
    "\n",
    "  Args:\n",
    "      y_true: Ground truth labels (one-hot encoded or integer).\n",
    "      y_pred: Predicted probabilities (2D array).\n",
    "      k: The value of k for top-k accuracy.\n",
    "\n",
    "  Returns:\n",
    "      Top-k accuracy as a float.\n",
    "  \"\"\"\n",
    "  correct = 0\n",
    "  for y_t, y_p in zip(y_true, y_pred):\n",
    "    # Get top k predictions (indices with highest scores)\n",
    "    top_k_indices = y_p.argsort()[-k:]\n",
    "\n",
    "    # Check if true label is in top k predictions\n",
    "    if y_t in top_k_indices:\n",
    "      correct += 1\n",
    "\n",
    "  accuracy = correct / len(y_true)\n",
    "  return accuracy, correct\n",
    "\n",
    "# Calculate top-k accuracy for k=5\n",
    "top_k_accuracy, top_k_accuracy_score = top_k_accuracy(y_true, y_pred, k=k)\n",
    "\n",
    "print(f\"Top-{k} accuracy: {top_k_accuracy}\")\n",
    "print(f\"Top-{k} accuracy: {top_k_accuracy_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71886b73-88b3-42a8-8750-706dec4369f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
