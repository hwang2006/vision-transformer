{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860ffd3f-deb6-4b5c-87fd-79d7dc4d6a51",
   "metadata": {},
   "source": [
    "#### [ML] ViT(20.10); Vision Transformer 코드 구현 및 설명 with pytorch\n",
    "\n",
    "\n",
    "\n",
    "https://kimbg.tistory.com/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f61c90-2c59-4b1e-8369-f9e33b73ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee83bed-d049-462b-94ee-ce87e588b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:10<00:00, 15881416.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data/\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
      "               RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "img_size = 224\n",
    "\n",
    "# Define image size\n",
    "image_size = (img_size, img_size)  # Replace with your desired image dimensions\n",
    "\n",
    "# Create data augmentation transforms\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor (CHW format)\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize pixel values (common practice)\n",
    "    transforms.Resize(image_size),  # Resize image to specified dimensions\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n",
    "    transforms.RandomRotation(degrees=(-15, 15)),  # Random rotation with range -15 to 15 degrees\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0), ratio=(0.75, 1.3333))  # Random resized crop\n",
    "])\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=data_augmentation)\n",
    "\n",
    "test_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=data_augmentation)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False)\n",
    "\n",
    "print(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7f62db-cd8a-45a3-8df0-a40f52672145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 3, 224, 224]) type: torch.FloatTensor\n",
      "Y_train: torch.Size([256]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (x, Y_train) in train_loader:\n",
    "    print(f\"X_train: {x.size()} type: {x.type()}\")\n",
    "    print(f\"Y_train: {Y_train.size()} type: {Y_train.type()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3747eff-64c4-4554-b7df-17bd3e702f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : torch.Size([256, 3, 224, 224])\n",
      "patches : torch.Size([256, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "## input ##\n",
    "print('x :', x.shape)\n",
    "\n",
    "patch_size = 16 # 16x16 pixel patch\n",
    "patches = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', \n",
    "                    s1=patch_size, s2=patch_size)\n",
    "print('patches :', patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0465544b-574a-4325-8449-bea155eafdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 14, 14]          98,432\n",
      "         Rearrange-2             [-1, 196, 128]               0\n",
      "================================================================\n",
      "Total params: 98,432\n",
      "Trainable params: 98,432\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 0.38\n",
      "Params size (MB): 0.38\n",
      "Estimated Total Size (MB): 1.33\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "patch_size = 16\n",
    "in_channels = 3\n",
    "#emb_size = 768 # channel * patch_size * patch_size\n",
    "emb_size = 128 # channel * patch_size * patch_size\n",
    "\n",
    "# using a conv layer instead of a linear one -> performance gains\n",
    "projection = nn.Sequential(\n",
    "    nn.Conv2d(in_channels, emb_size, \n",
    "              kernel_size=patch_size, stride=patch_size), # torch.Size([8, 768, 14, 14])\n",
    "    Rearrange('b e (h) (w) -> b (h w) e'))\n",
    "\n",
    "summary(projection, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f654ad23-29e4-4676-91bf-71b8cab25017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected X shape : torch.Size([256, 196, 128])\n",
      "Cls Shape : torch.Size([1, 1, 128]) , Pos Shape : torch.Size([197, 128])\n",
      "Repeated Cls shape : torch.Size([256, 1, 128])\n",
      "output :  torch.Size([256, 197, 128])\n"
     ]
    }
   ],
   "source": [
    "img_size = 224\n",
    "patch_size = 16\n",
    "\n",
    "# 이미지를 패치사이즈로 나누고 flatten\n",
    "projected_x = projection(x)\n",
    "print('Projected X shape :', projected_x.shape)\n",
    "\n",
    "# cls_token과 pos encoding Parameter 정의\n",
    "cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "print('Cls Shape :', cls_token.shape, ', Pos Shape :', positions.shape)\n",
    "\n",
    "# cls_token을 반복하여 배치사이즈의 크기와 맞춰줌\n",
    "batch_size = BATCH_SIZE\n",
    "cls_tokens = repeat(cls_token, '() n e -> b n e', b=batch_size)\n",
    "print('Repeated Cls shape :', cls_tokens.shape)\n",
    "\n",
    "# cls_token과 projected_x를 concatenate\n",
    "cat_x = torch.cat([cls_tokens, projected_x], dim=1)\n",
    "\n",
    "# position encoding을 더해줌\n",
    "cat_x += positions\n",
    "print('output : ', cat_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f25af137-3eaa-4dac-8329-db30f6f20ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 14, 14]          98,432\n",
      "         Rearrange-2             [-1, 196, 128]               0\n",
      "================================================================\n",
      "Total params: 98,432\n",
      "Trainable params: 98,432\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 0.38\n",
      "Params size (MB): 0.38\n",
      "Estimated Total Size (MB): 1.33\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = patch_size, \n",
    "                 emb_size: int = emb_size, img_size: int = img_size):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            # using a conv layer instead of a linear one -> performance gains\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        # prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        # add position embedding\n",
    "        x += self.positions\n",
    "\n",
    "        return x\n",
    "        \n",
    "PE = PatchEmbedding()\n",
    "summary(PE, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51bb136a-05f0-4c87-a5b5-ff0c0c8d9a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 3, 224, 224]) type: torch.FloatTensor\n",
      "Y_train: torch.Size([256]) type: torch.LongTensor\n",
      "Linear(in_features=128, out_features=128, bias=True) Linear(in_features=128, out_features=128, bias=True) Linear(in_features=128, out_features=128, bias=True)\n",
      "torch.Size([256, 197, 128])\n",
      "torch.Size([256, 197, 128])\n",
      "shape : torch.Size([256, 8, 197, 16]) torch.Size([256, 8, 197, 16]) torch.Size([256, 8, 197, 16])\n"
     ]
    }
   ],
   "source": [
    "for (x, Y_train) in train_loader:\n",
    "    print(f\"X_train: {x.size()} type: {x.type()}\")\n",
    "    print(f\"Y_train: {Y_train.size()} type: {Y_train.type()}\")\n",
    "    break\n",
    "    \n",
    "#emb_size = 768\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "keys = nn.Linear(emb_size, emb_size)\n",
    "queries = nn.Linear(emb_size, emb_size)\n",
    "values = nn.Linear(emb_size, emb_size)\n",
    "print(keys, queries, values)\n",
    "\n",
    "x = PE(x)\n",
    "print(x.shape)\n",
    "print(queries(x).shape) # batch, n, emb_size\n",
    "queries = rearrange(queries(x), \"b n (h d) -> b h n d\", h=num_heads) # -> batch, head, n, emb_size/head\n",
    "keys = rearrange(keys(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "values  = rearrange(values(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "\n",
    "print('shape :', queries.shape, keys.shape, values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "104553b3-654f-4256-ba02-e72a8a43f200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy : torch.Size([256, 8, 197, 197])\n",
      "att : torch.Size([256, 8, 197, 197])\n",
      "out : torch.Size([256, 8, 197, 16])\n",
      "out2 :  torch.Size([256, 197, 128])\n"
     ]
    }
   ],
   "source": [
    "# Queries * Keys\n",
    "energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "print('energy :', energy.shape)\n",
    "\n",
    "# Get Attention Score\n",
    "scaling = emb_size ** (1/2)\n",
    "att = F.softmax(energy/scaling, dim=-1) \n",
    "print('att :', att.shape)\n",
    "\n",
    "# Attention Score * values\n",
    "out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "print('out :', out.shape)\n",
    "\n",
    "# Rearrage to emb_size\n",
    "out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "print('out2 : ', out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdc4f1a8-33f6-47e4-8933-455ec8d0aacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 3, 224, 224]) type: torch.FloatTensor\n",
      "Y_train: torch.Size([256]) type: torch.LongTensor\n",
      "torch.Size([256, 197, 128])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 197, 384]          49,536\n",
      "           Dropout-2          [-1, 8, 197, 197]               0\n",
      "            Linear-3             [-1, 197, 128]          16,512\n",
      "================================================================\n",
      "Total params: 66,048\n",
      "Trainable params: 66,048\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.10\n",
      "Forward/backward pass size (MB): 3.14\n",
      "Params size (MB): 0.25\n",
      "Estimated Total Size (MB): 3.49\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = emb_size, num_heads: int = num_heads, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "            \n",
    "        scaling = self.emb_size ** (1/2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "        \n",
    "for (x, Y_train) in train_loader:\n",
    "    print(f\"X_train: {x.size()} type: {x.type()}\")\n",
    "    print(f\"Y_train: {Y_train.size()} type: {Y_train.type()}\")\n",
    "    break       \n",
    "\n",
    "PE = PatchEmbedding()\n",
    "x = PE(x)\n",
    "print(x.shape)\n",
    "MHA = MultiHeadAttention()\n",
    "summary(MHA, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25de9cd2-d112-4c43-bf7a-29a8ad79cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 3, 224, 224]) type: torch.FloatTensor\n",
      "Y_train: torch.Size([256]) type: torch.LongTensor\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1             [-1, 197, 128]             256\n",
      "            Linear-2             [-1, 197, 384]          49,536\n",
      "           Dropout-3          [-1, 8, 197, 197]               0\n",
      "            Linear-4             [-1, 197, 128]          16,512\n",
      "MultiHeadAttention-5             [-1, 197, 128]               0\n",
      "           Dropout-6             [-1, 197, 128]               0\n",
      "       ResidualAdd-7             [-1, 197, 128]               0\n",
      "         LayerNorm-8             [-1, 197, 128]             256\n",
      "            Linear-9             [-1, 197, 512]          66,048\n",
      "             GELU-10             [-1, 197, 512]               0\n",
      "          Dropout-11             [-1, 197, 512]               0\n",
      "           Linear-12             [-1, 197, 128]          65,664\n",
      "          Dropout-13             [-1, 197, 128]               0\n",
      "      ResidualAdd-14             [-1, 197, 128]               0\n",
      "================================================================\n",
      "Total params: 198,272\n",
      "Trainable params: 198,272\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.10\n",
      "Forward/backward pass size (MB): 6.99\n",
      "Params size (MB): 0.76\n",
      "Estimated Total Size (MB): 7.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = emb_size,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "#x = torch.randn(8, 3, 224, 224)\n",
    "for (x, Y_train) in train_loader:\n",
    "    print(f\"X_train: {x.size()} type: {x.type()}\")\n",
    "    print(f\"Y_train: {Y_train.size()} type: {Y_train.type()}\")\n",
    "    break       \n",
    "x = PE(x)\n",
    "x = MHA(x)\n",
    "TE = TransformerEncoderBlock()\n",
    "summary(TE, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87714444-b121-4bfa-ab1d-a471abbeff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "    PatchEmbedding-3             [-1, 197, 768]               0\n",
      "         LayerNorm-4             [-1, 197, 768]           1,536\n",
      "            Linear-5            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-6          [-1, 8, 197, 197]               0\n",
      "            Linear-7             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-8             [-1, 197, 768]               0\n",
      "           Dropout-9             [-1, 197, 768]               0\n",
      "      ResidualAdd-10             [-1, 197, 768]               0\n",
      "        LayerNorm-11             [-1, 197, 768]           1,536\n",
      "           Linear-12            [-1, 197, 3072]       2,362,368\n",
      "             GELU-13            [-1, 197, 3072]               0\n",
      "          Dropout-14            [-1, 197, 3072]               0\n",
      "           Linear-15             [-1, 197, 768]       2,360,064\n",
      "          Dropout-16             [-1, 197, 768]               0\n",
      "      ResidualAdd-17             [-1, 197, 768]               0\n",
      "        LayerNorm-18             [-1, 197, 768]           1,536\n",
      "           Linear-19            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-20          [-1, 8, 197, 197]               0\n",
      "           Linear-21             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-22             [-1, 197, 768]               0\n",
      "          Dropout-23             [-1, 197, 768]               0\n",
      "      ResidualAdd-24             [-1, 197, 768]               0\n",
      "        LayerNorm-25             [-1, 197, 768]           1,536\n",
      "           Linear-26            [-1, 197, 3072]       2,362,368\n",
      "             GELU-27            [-1, 197, 3072]               0\n",
      "          Dropout-28            [-1, 197, 3072]               0\n",
      "           Linear-29             [-1, 197, 768]       2,360,064\n",
      "          Dropout-30             [-1, 197, 768]               0\n",
      "      ResidualAdd-31             [-1, 197, 768]               0\n",
      "        LayerNorm-32             [-1, 197, 768]           1,536\n",
      "           Linear-33            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-34          [-1, 8, 197, 197]               0\n",
      "           Linear-35             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-36             [-1, 197, 768]               0\n",
      "          Dropout-37             [-1, 197, 768]               0\n",
      "      ResidualAdd-38             [-1, 197, 768]               0\n",
      "        LayerNorm-39             [-1, 197, 768]           1,536\n",
      "           Linear-40            [-1, 197, 3072]       2,362,368\n",
      "             GELU-41            [-1, 197, 3072]               0\n",
      "          Dropout-42            [-1, 197, 3072]               0\n",
      "           Linear-43             [-1, 197, 768]       2,360,064\n",
      "          Dropout-44             [-1, 197, 768]               0\n",
      "      ResidualAdd-45             [-1, 197, 768]               0\n",
      "        LayerNorm-46             [-1, 197, 768]           1,536\n",
      "           Linear-47            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-48          [-1, 8, 197, 197]               0\n",
      "           Linear-49             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-50             [-1, 197, 768]               0\n",
      "          Dropout-51             [-1, 197, 768]               0\n",
      "      ResidualAdd-52             [-1, 197, 768]               0\n",
      "        LayerNorm-53             [-1, 197, 768]           1,536\n",
      "           Linear-54            [-1, 197, 3072]       2,362,368\n",
      "             GELU-55            [-1, 197, 3072]               0\n",
      "          Dropout-56            [-1, 197, 3072]               0\n",
      "           Linear-57             [-1, 197, 768]       2,360,064\n",
      "          Dropout-58             [-1, 197, 768]               0\n",
      "      ResidualAdd-59             [-1, 197, 768]               0\n",
      "        LayerNorm-60             [-1, 197, 768]           1,536\n",
      "           Linear-61            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-62          [-1, 8, 197, 197]               0\n",
      "           Linear-63             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-64             [-1, 197, 768]               0\n",
      "          Dropout-65             [-1, 197, 768]               0\n",
      "      ResidualAdd-66             [-1, 197, 768]               0\n",
      "        LayerNorm-67             [-1, 197, 768]           1,536\n",
      "           Linear-68            [-1, 197, 3072]       2,362,368\n",
      "             GELU-69            [-1, 197, 3072]               0\n",
      "          Dropout-70            [-1, 197, 3072]               0\n",
      "           Linear-71             [-1, 197, 768]       2,360,064\n",
      "          Dropout-72             [-1, 197, 768]               0\n",
      "      ResidualAdd-73             [-1, 197, 768]               0\n",
      "        LayerNorm-74             [-1, 197, 768]           1,536\n",
      "           Linear-75            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-76          [-1, 8, 197, 197]               0\n",
      "           Linear-77             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-78             [-1, 197, 768]               0\n",
      "          Dropout-79             [-1, 197, 768]               0\n",
      "      ResidualAdd-80             [-1, 197, 768]               0\n",
      "        LayerNorm-81             [-1, 197, 768]           1,536\n",
      "           Linear-82            [-1, 197, 3072]       2,362,368\n",
      "             GELU-83            [-1, 197, 3072]               0\n",
      "          Dropout-84            [-1, 197, 3072]               0\n",
      "           Linear-85             [-1, 197, 768]       2,360,064\n",
      "          Dropout-86             [-1, 197, 768]               0\n",
      "      ResidualAdd-87             [-1, 197, 768]               0\n",
      "        LayerNorm-88             [-1, 197, 768]           1,536\n",
      "           Linear-89            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-90          [-1, 8, 197, 197]               0\n",
      "           Linear-91             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-92             [-1, 197, 768]               0\n",
      "          Dropout-93             [-1, 197, 768]               0\n",
      "      ResidualAdd-94             [-1, 197, 768]               0\n",
      "        LayerNorm-95             [-1, 197, 768]           1,536\n",
      "           Linear-96            [-1, 197, 3072]       2,362,368\n",
      "             GELU-97            [-1, 197, 3072]               0\n",
      "          Dropout-98            [-1, 197, 3072]               0\n",
      "           Linear-99             [-1, 197, 768]       2,360,064\n",
      "         Dropout-100             [-1, 197, 768]               0\n",
      "     ResidualAdd-101             [-1, 197, 768]               0\n",
      "       LayerNorm-102             [-1, 197, 768]           1,536\n",
      "          Linear-103            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-104          [-1, 8, 197, 197]               0\n",
      "          Linear-105             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-106             [-1, 197, 768]               0\n",
      "         Dropout-107             [-1, 197, 768]               0\n",
      "     ResidualAdd-108             [-1, 197, 768]               0\n",
      "       LayerNorm-109             [-1, 197, 768]           1,536\n",
      "          Linear-110            [-1, 197, 3072]       2,362,368\n",
      "            GELU-111            [-1, 197, 3072]               0\n",
      "         Dropout-112            [-1, 197, 3072]               0\n",
      "          Linear-113             [-1, 197, 768]       2,360,064\n",
      "         Dropout-114             [-1, 197, 768]               0\n",
      "     ResidualAdd-115             [-1, 197, 768]               0\n",
      "       LayerNorm-116             [-1, 197, 768]           1,536\n",
      "          Linear-117            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-118          [-1, 8, 197, 197]               0\n",
      "          Linear-119             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-120             [-1, 197, 768]               0\n",
      "         Dropout-121             [-1, 197, 768]               0\n",
      "     ResidualAdd-122             [-1, 197, 768]               0\n",
      "       LayerNorm-123             [-1, 197, 768]           1,536\n",
      "          Linear-124            [-1, 197, 3072]       2,362,368\n",
      "            GELU-125            [-1, 197, 3072]               0\n",
      "         Dropout-126            [-1, 197, 3072]               0\n",
      "          Linear-127             [-1, 197, 768]       2,360,064\n",
      "         Dropout-128             [-1, 197, 768]               0\n",
      "     ResidualAdd-129             [-1, 197, 768]               0\n",
      "       LayerNorm-130             [-1, 197, 768]           1,536\n",
      "          Linear-131            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-132          [-1, 8, 197, 197]               0\n",
      "          Linear-133             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-134             [-1, 197, 768]               0\n",
      "         Dropout-135             [-1, 197, 768]               0\n",
      "     ResidualAdd-136             [-1, 197, 768]               0\n",
      "       LayerNorm-137             [-1, 197, 768]           1,536\n",
      "          Linear-138            [-1, 197, 3072]       2,362,368\n",
      "            GELU-139            [-1, 197, 3072]               0\n",
      "         Dropout-140            [-1, 197, 3072]               0\n",
      "          Linear-141             [-1, 197, 768]       2,360,064\n",
      "         Dropout-142             [-1, 197, 768]               0\n",
      "     ResidualAdd-143             [-1, 197, 768]               0\n",
      "       LayerNorm-144             [-1, 197, 768]           1,536\n",
      "          Linear-145            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-146          [-1, 8, 197, 197]               0\n",
      "          Linear-147             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-148             [-1, 197, 768]               0\n",
      "         Dropout-149             [-1, 197, 768]               0\n",
      "     ResidualAdd-150             [-1, 197, 768]               0\n",
      "       LayerNorm-151             [-1, 197, 768]           1,536\n",
      "          Linear-152            [-1, 197, 3072]       2,362,368\n",
      "            GELU-153            [-1, 197, 3072]               0\n",
      "         Dropout-154            [-1, 197, 3072]               0\n",
      "          Linear-155             [-1, 197, 768]       2,360,064\n",
      "         Dropout-156             [-1, 197, 768]               0\n",
      "     ResidualAdd-157             [-1, 197, 768]               0\n",
      "       LayerNorm-158             [-1, 197, 768]           1,536\n",
      "          Linear-159            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-160          [-1, 8, 197, 197]               0\n",
      "          Linear-161             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-162             [-1, 197, 768]               0\n",
      "         Dropout-163             [-1, 197, 768]               0\n",
      "     ResidualAdd-164             [-1, 197, 768]               0\n",
      "       LayerNorm-165             [-1, 197, 768]           1,536\n",
      "          Linear-166            [-1, 197, 3072]       2,362,368\n",
      "            GELU-167            [-1, 197, 3072]               0\n",
      "         Dropout-168            [-1, 197, 3072]               0\n",
      "          Linear-169             [-1, 197, 768]       2,360,064\n",
      "         Dropout-170             [-1, 197, 768]               0\n",
      "     ResidualAdd-171             [-1, 197, 768]               0\n",
      "          Reduce-172                  [-1, 768]               0\n",
      "       LayerNorm-173                  [-1, 768]           1,536\n",
      "          Linear-174                  [-1, 100]          76,900\n",
      "================================================================\n",
      "Total params: 85,723,492\n",
      "Trainable params: 85,723,492\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 364.33\n",
      "Params size (MB): 327.01\n",
      "Estimated Total Size (MB): 691.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 768, n_classes: int = num_classes):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))\n",
    "\n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 768,\n",
    "                img_size: int = 224,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = num_classes,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n",
    "        \n",
    "summary(ViT(), (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddeaa91d-806e-4523-b4da-d0dc3a6347d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 3, 224, 224]) type: torch.FloatTensor\n",
      "Y_train: torch.Size([256]) type: torch.LongTensor\n",
      "torch.Size([256, 100])\n"
     ]
    }
   ],
   "source": [
    "for (x, Y_train) in train_loader:\n",
    "    print(f\"X_train: {x.size()} type: {x.type()}\")\n",
    "    print(f\"Y_train: {Y_train.size()} type: {Y_train.type()}\")\n",
    "    break    \n",
    "\n",
    "vit = ViT()\n",
    "out = vit(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62642413-ea73-43ea-8dd8-d57fe8e54718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
      "               RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.8, 1.2), interpolation=bilinear, antialias=True)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "img_size = 224\n",
    "\n",
    "# Define image size\n",
    "image_size = (img_size, img_size)  # Replace with your desired image dimensions\n",
    "\n",
    "# Create data augmentation transforms\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor (CHW format)\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize pixel values (common practice)\n",
    "    transforms.Resize(image_size),  # Resize image to specified dimensions\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n",
    "    transforms.RandomRotation(degrees=(-15, 15)),  # Random rotation with range -15 to 15 degrees\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0), ratio=(0.8, 1.2))  # Random resized crop\n",
    "])\n",
    "\n",
    "test_preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor (CHW format)\n",
    "    transforms.Resize(image_size),  # Resize image to specified dimensions \n",
    "])\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "#BATCH_SIZE = 512\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=data_augmentation)\n",
    "\n",
    "test_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=data_augmentation,\n",
    "                                #transform=test_preprocess,\n",
    "                                )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           #num_workers=4,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          #num_workers=4,\n",
    "                                          shuffle=False)\n",
    "\n",
    "print(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eeb8e7-22fa-44e7-9cc5-31e6b8169e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9619f117784b25b6b92182be30d20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Epoch: 1 [0/50000(0%)]                            \tTrain Loss: 4.803012371063232                            \tTrain Accuracy: 0.00%\n",
      "train Epoch: 1 [5120/50000(10%)]                            \tTrain Loss: 4.446662425994873                            \tTrain Accuracy: 3.12%\n",
      "train Epoch: 1 [10240/50000(20%)]                            \tTrain Loss: 4.35072660446167                            \tTrain Accuracy: 2.73%\n",
      "train Epoch: 1 [15360/50000(31%)]                            \tTrain Loss: 4.451959609985352                            \tTrain Accuracy: 2.34%\n",
      "train Epoch: 1 [20480/50000(41%)]                            \tTrain Loss: 4.363228797912598                            \tTrain Accuracy: 3.52%\n",
      "train Epoch: 1 [25600/50000(51%)]                            \tTrain Loss: 4.247077465057373                            \tTrain Accuracy: 4.30%\n",
      "train Epoch: 1 [30720/50000(61%)]                            \tTrain Loss: 4.247827053070068                            \tTrain Accuracy: 4.69%\n",
      "train Epoch: 1 [35840/50000(71%)]                            \tTrain Loss: 4.082474708557129                            \tTrain Accuracy: 8.98%\n",
      "train Epoch: 1 [40960/50000(82%)]                            \tTrain Loss: 3.9487805366516113                            \tTrain Accuracy: 7.42%\n",
      "train Epoch: 1 [46080/50000(92%)]                            \tTrain Loss: 3.9853851795196533                            \tTrain Accuracy: 6.25%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d3c939f39e412dbc6d5001c825780a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH: 1]\tTest Loss: 0.0158\tTest Accuracy: 8.49 %\tTest top k Accuracy: 27.79 % \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2551c5808c7842988e5fca956247c0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Epoch: 2 [0/50000(0%)]                            \tTrain Loss: 3.971151113510132                            \tTrain Accuracy: 9.38%\n",
      "train Epoch: 2 [5120/50000(10%)]                            \tTrain Loss: 3.9428086280822754                            \tTrain Accuracy: 8.98%\n",
      "train Epoch: 2 [10240/50000(20%)]                            \tTrain Loss: 3.9338865280151367                            \tTrain Accuracy: 12.50%\n",
      "train Epoch: 2 [15360/50000(31%)]                            \tTrain Loss: 3.9513208866119385                            \tTrain Accuracy: 12.50%\n",
      "train Epoch: 2 [20480/50000(41%)]                            \tTrain Loss: 3.8655827045440674                            \tTrain Accuracy: 9.77%\n",
      "train Epoch: 2 [25600/50000(51%)]                            \tTrain Loss: 3.6936087608337402                            \tTrain Accuracy: 10.55%\n",
      "train Epoch: 2 [30720/50000(61%)]                            \tTrain Loss: 3.7450931072235107                            \tTrain Accuracy: 9.38%\n",
      "train Epoch: 2 [35840/50000(71%)]                            \tTrain Loss: 3.8870067596435547                            \tTrain Accuracy: 9.77%\n",
      "train Epoch: 2 [40960/50000(82%)]                            \tTrain Loss: 3.7909631729125977                            \tTrain Accuracy: 12.89%\n",
      "train Epoch: 2 [46080/50000(92%)]                            \tTrain Loss: 3.6790719032287598                            \tTrain Accuracy: 13.28%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b5a079806a4e3c87d99215860833ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH: 2]\tTest Loss: 0.0147\tTest Accuracy: 12.98 %\tTest top k Accuracy: 35.63 % \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7029b60973c4d34a8a8ed17ac113bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Epoch: 3 [0/50000(0%)]                            \tTrain Loss: 3.6462349891662598                            \tTrain Accuracy: 13.28%\n",
      "train Epoch: 3 [5120/50000(10%)]                            \tTrain Loss: 3.5719211101531982                            \tTrain Accuracy: 14.06%\n",
      "train Epoch: 3 [10240/50000(20%)]                            \tTrain Loss: 3.6494131088256836                            \tTrain Accuracy: 10.94%\n",
      "train Epoch: 3 [15360/50000(31%)]                            \tTrain Loss: 3.6879851818084717                            \tTrain Accuracy: 12.11%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Assuming your model is defined in `model`\n",
    "\n",
    "num_epochs = 15\n",
    "weight_decay = 0.001\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = ViT().to(DEVICE)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Define loss function (assuming model outputs logits)\n",
    "criterion = nn.CrossEntropyLoss()  # For logits\n",
    "\n",
    "def top_k_accuracy_score(y_true, y_pred, k):\n",
    "  \"\"\"\n",
    "  Calculates top-k accuracy score.\n",
    "\n",
    "  Args:\n",
    "      y_true: Ground truth labels (one-hot encoded or integer).\n",
    "      y_pred: Predicted probabilities (2D array).\n",
    "      k: The value of k for top-k accuracy.\n",
    "\n",
    "  Returns:\n",
    "      Top-k accuracy score.\n",
    "  \"\"\"\n",
    "  correct = 0\n",
    "  for y_t, y_p in zip(y_true, y_pred):\n",
    "    # Get top k predictions (indices with highest scores)\n",
    "    top_k_indices = y_p.argsort()[-k:]\n",
    "\n",
    "    # Check if true label is in top k predictions\n",
    "    if y_t in top_k_indices:\n",
    "      correct += 1\n",
    "\n",
    "  #accuracy = correct / len(y_true)\n",
    "  return correct\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in tqdm(enumerate(train_loader)):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            correct = top_k_accuracy_score(label.clone(), output.clone(), k=1) \n",
    "            #accuracy = correct / len(label)\n",
    "            print(\n",
    "                f\"train Epoch: {Epoch} [{batch_idx * len(image)}/{len(train_loader.dataset)}({100. * batch_idx / len(train_loader):.0f}%)] \\\n",
    "                           \\tTrain Loss: {loss.item()} \\\n",
    "                           \\tTrain Accuracy: {100. * correct / len(label):.2f}%\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    top_k_correct = 0 \n",
    "    #print(f\"test_loader length : {len(test_loader.dataset)}\") #10000\n",
    "    with torch.no_grad():\n",
    "        for image, label in tqdm(test_loader):\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            #print(f\"output.shape: {output.shape}\") #output.shape: torch.Size([512, 100])\n",
    "            #print(f\"label.shape: {label.shape}\") #label.shape: torch.Size([512])\n",
    "            \n",
    "            test_loss += criterion(output, label).item()\n",
    "            #prediction = output.max(1, keepdim=True)[1] \n",
    "            #print(prediction.shape) # torch.Size([512, 1])\n",
    "            #correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            correct += top_k_accuracy_score(label, output, k=1)\n",
    "            #print(f\"correct: {correct}\")\n",
    "            top_k_correct += top_k_accuracy_score(label, output, k=5)\n",
    "            #print(f\"top_k_correct: {top_k_correct}\")\n",
    "\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    test_top_k_accuracy = 100. * top_k_correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy, test_top_k_accuracy\n",
    "\n",
    "for Epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, optimizer, log_interval=20)\n",
    "    test_loss, test_accuracy, test_top_k_accuracy = evaluate(model, test_loader)\n",
    "    print(f\"\\n[EPOCH: {Epoch}]\\tTest Loss: {test_loss:.4f}\\tTest Accuracy: {test_accuracy} %\\tTest top k Accuracy: {test_top_k_accuracy} % \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd375ad-3d3f-4046-aae6-c5363c31f70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
