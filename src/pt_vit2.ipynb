{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860ffd3f-deb6-4b5c-87fd-79d7dc4d6a51",
   "metadata": {},
   "source": [
    "#### [ML] ViT(20.10); Vision Transformer 코드 구현 및 설명 with pytorch\n",
    "\n",
    "\n",
    "\n",
    "https://kimbg.tistory.com/31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6c8b5-78b1-4500-ba5e-37474fcd2079",
   "metadata": {},
   "source": [
    "#### Another Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba91988d-7c8c-477f-9edf-94b06c92b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 96, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', \n",
    "                 channels = 3, dim_head = 96, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41edab6b-b420-42e7-a2a9-f203448cf8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "#import torch.nn.functional as F\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#from torch import nn\n",
    "#from torch import Tensor\n",
    "#from PIL import Image\n",
    "#from torchvision.transforms import Compose, Resize, ToTensor\n",
    "#from einops import rearrange, reduce, repeat\n",
    "#from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "img_size = 224\n",
    "\n",
    "# Define image size\n",
    "image_size = (img_size, img_size)  # Replace with your desired image dimensions\n",
    "\n",
    "# Create data augmentation transforms\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor (CHW format)\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize pixel values (common practice)\n",
    "    transforms.Resize(image_size),  # Resize image to specified dimensions\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n",
    "    transforms.RandomRotation(degrees=(-15, 15)),  # Random rotation with range -15 to 15 degrees\n",
    "    #transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0), ratio=(0.75, 1.3333))  # Random resized crop\n",
    "])\n",
    "\n",
    "#BATCH_SIZE = 512\n",
    "BATCH_SIZE = 256\n",
    "train_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=data_augmentation)\n",
    "\n",
    "test_dataset = datasets.CIFAR100(root=\"./data/\",\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=data_augmentation)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False)\n",
    "\n",
    "print(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e25446-51b2-4407-8819-7a8eb7989f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size = 224,\n",
    "    patch_size = 16,\n",
    "    #num_classes = 1000,\n",
    "    num_classes = 100, #cifar100\n",
    "    #dim = 768,\n",
    "    dim = 64,\n",
    "    depth = 12,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "#summary(model, (3,224,224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe3f18b-fdaf-407e-a83f-b34ec7d1323f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9b9fa617bd464d9de1ac392f9d8ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Epoch: 1 [0/50000(0%)]                            \tTrain Loss: 4.6979875564575195                            \tTrain Accuracy: 0.78%\n",
      "train Epoch: 1 [5120/50000(10%)]                            \tTrain Loss: 4.534120559692383                            \tTrain Accuracy: 3.52%\n",
      "train Epoch: 1 [10240/50000(20%)]                            \tTrain Loss: 4.3945536613464355                            \tTrain Accuracy: 3.12%\n",
      "train Epoch: 1 [15360/50000(31%)]                            \tTrain Loss: 4.277237892150879                            \tTrain Accuracy: 4.30%\n",
      "train Epoch: 1 [20480/50000(41%)]                            \tTrain Loss: 4.18536376953125                            \tTrain Accuracy: 3.91%\n",
      "train Epoch: 1 [25600/50000(51%)]                            \tTrain Loss: 4.1563920974731445                            \tTrain Accuracy: 5.47%\n",
      "train Epoch: 1 [30720/50000(61%)]                            \tTrain Loss: 4.116108417510986                            \tTrain Accuracy: 5.47%\n",
      "train Epoch: 1 [35840/50000(71%)]                            \tTrain Loss: 4.123956203460693                            \tTrain Accuracy: 5.08%\n",
      "train Epoch: 1 [40960/50000(82%)]                            \tTrain Loss: 4.046113014221191                            \tTrain Accuracy: 8.20%\n",
      "train Epoch: 1 [46080/50000(92%)]                            \tTrain Loss: 3.926802396774292                            \tTrain Accuracy: 8.59%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b58747427c47fca209955dcef6b5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH: 1]\tTest Loss: 0.0157\tTest Accuracy: 9.5 %\tTest top k Accuracy: 28.77 % \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246f420082224b33ab6b41c88ac1145b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Epoch: 2 [0/50000(0%)]                            \tTrain Loss: 3.8715267181396484                            \tTrain Accuracy: 12.11%\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "#from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Assuming your model is defined in `model`\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "weight_decay = 0.001\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Define loss function (assuming model outputs logits)\n",
    "criterion = nn.CrossEntropyLoss()  # For logits\n",
    "\n",
    "def top_k_accuracy_score(y_true, y_pred, k):\n",
    "  \"\"\"\n",
    "  Calculates top-k accuracy score.\n",
    "\n",
    "  Args:\n",
    "      y_true: Ground truth labels (one-hot encoded or integer).\n",
    "      y_pred: Predicted probabilities (2D array).\n",
    "      k: The value of k for top-k accuracy.\n",
    "\n",
    "  Returns:\n",
    "      Top-k accuracy score.\n",
    "  \"\"\"\n",
    "  correct = 0\n",
    "  for y_t, y_p in zip(y_true, y_pred):\n",
    "    # Get top k predictions (indices with highest scores)\n",
    "    top_k_indices = y_p.argsort()[-k:]\n",
    "\n",
    "    # Check if true label is in top k predictions\n",
    "    if y_t in top_k_indices:\n",
    "      correct += 1\n",
    "\n",
    "  #accuracy = correct / len(y_true)\n",
    "  return correct\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in tqdm(enumerate(train_loader)):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            correct = top_k_accuracy_score(label.clone(), output.clone(), k=1) \n",
    "            #accuracy = correct / len(label)\n",
    "            print(\n",
    "                f\"train Epoch: {Epoch} [{batch_idx * len(image)}/{len(train_loader.dataset)}({100. * batch_idx / len(train_loader):.0f}%)] \\\n",
    "                           \\tTrain Loss: {loss.item()} \\\n",
    "                           \\tTrain Accuracy: {100. * correct / len(label):.2f}%\")\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    top_k_correct = 0 \n",
    "    with torch.no_grad():\n",
    "        for image, label in tqdm(test_loader):\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            #print(f\"output.shape: {output.shape}\") #output.shape: torch.Size([512, 100])\n",
    "            #print(f\"label.shape: {label.shape}\") #label.shape: torch.Size([512])\n",
    "            \n",
    "            test_loss += criterion(output, label).item()\n",
    "            #prediction = output.max(1, keepdim=True)[1]\n",
    "            #print(prediction.shape) # torch.Size([512, 1])\n",
    "            #correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            correct += top_k_accuracy_score(label, output, k=1)\n",
    "            #print(f\"correct: {correct}\")\n",
    "            top_k_correct += top_k_accuracy_score(label, output, k=5)\n",
    "            #print(f\"top_k_correct: {top_k_correct}\")\n",
    "\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    test_top_k_accuracy = 100. * top_k_correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy, test_top_k_accuracy\n",
    "\n",
    "for Epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, optimizer, log_interval=20)\n",
    "    test_loss, test_accuracy, test_top_k_accuracy = evaluate(model, test_loader)\n",
    "    print(f\"\\n[EPOCH: {Epoch}]\\tTest Loss: {test_loss:.4f}\\tTest Accuracy: {test_accuracy} %\\tTest top k Accuracy: {test_top_k_accuracy} % \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f663c22b-3a35-430a-8162-07a36a4fa9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
